{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "import torch\n",
    "\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "torch.manual_seed(25) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"behrad3d/nasa-cmaps\")\n",
    "\n",
    "train1_path = path + \"/CMaps/train_FD001.txt\"\n",
    "test1_path = path + \"/CMaps/test_FD001.txt\"\n",
    "rul1_path = path + \"/CMaps/RUL_FD001.txt\"\n",
    "data_train = pd.read_csv(train1_path, sep = \" \", header=None)\n",
    "data_test = pd.read_csv(test1_path, sep = \" \", header=None)\n",
    "data_rul = pd.read_csv(rul1_path)\n",
    "\n",
    "train_copy = data_train\n",
    "test_copy = data_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Yoni\\.cache\\kagglehub\\datasets\\behrad3d\\nasa-cmaps\\versions\\1\n"
     ]
    }
   ],
   "source": [
    "print(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.0007</td>\n",
       "      <td>-0.0004</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>641.82</td>\n",
       "      <td>1589.70</td>\n",
       "      <td>1400.60</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>8138.62</td>\n",
       "      <td>8.4195</td>\n",
       "      <td>0.03</td>\n",
       "      <td>392</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>39.06</td>\n",
       "      <td>23.4190</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0019</td>\n",
       "      <td>-0.0003</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>642.15</td>\n",
       "      <td>1591.82</td>\n",
       "      <td>1403.14</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>8131.49</td>\n",
       "      <td>8.4318</td>\n",
       "      <td>0.03</td>\n",
       "      <td>392</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>39.00</td>\n",
       "      <td>23.4236</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>-0.0043</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>642.35</td>\n",
       "      <td>1587.99</td>\n",
       "      <td>1404.20</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>8133.23</td>\n",
       "      <td>8.4178</td>\n",
       "      <td>0.03</td>\n",
       "      <td>390</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>38.95</td>\n",
       "      <td>23.3442</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0007</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>642.35</td>\n",
       "      <td>1582.79</td>\n",
       "      <td>1401.87</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>8133.83</td>\n",
       "      <td>8.3682</td>\n",
       "      <td>0.03</td>\n",
       "      <td>392</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>38.88</td>\n",
       "      <td>23.3739</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>-0.0019</td>\n",
       "      <td>-0.0002</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>642.37</td>\n",
       "      <td>1582.85</td>\n",
       "      <td>1406.22</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>8133.80</td>\n",
       "      <td>8.4294</td>\n",
       "      <td>0.03</td>\n",
       "      <td>393</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>38.90</td>\n",
       "      <td>23.4044</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20626</th>\n",
       "      <td>100</td>\n",
       "      <td>196</td>\n",
       "      <td>-0.0004</td>\n",
       "      <td>-0.0003</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>643.49</td>\n",
       "      <td>1597.98</td>\n",
       "      <td>1428.63</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>8137.60</td>\n",
       "      <td>8.4956</td>\n",
       "      <td>0.03</td>\n",
       "      <td>397</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>38.49</td>\n",
       "      <td>22.9735</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20627</th>\n",
       "      <td>100</td>\n",
       "      <td>197</td>\n",
       "      <td>-0.0016</td>\n",
       "      <td>-0.0005</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>643.54</td>\n",
       "      <td>1604.50</td>\n",
       "      <td>1433.58</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>8136.50</td>\n",
       "      <td>8.5139</td>\n",
       "      <td>0.03</td>\n",
       "      <td>395</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>38.30</td>\n",
       "      <td>23.1594</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20628</th>\n",
       "      <td>100</td>\n",
       "      <td>198</td>\n",
       "      <td>0.0004</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>643.42</td>\n",
       "      <td>1602.46</td>\n",
       "      <td>1428.18</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>8141.05</td>\n",
       "      <td>8.5646</td>\n",
       "      <td>0.03</td>\n",
       "      <td>398</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>38.44</td>\n",
       "      <td>22.9333</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20629</th>\n",
       "      <td>100</td>\n",
       "      <td>199</td>\n",
       "      <td>-0.0011</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>643.23</td>\n",
       "      <td>1605.26</td>\n",
       "      <td>1426.53</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>8139.29</td>\n",
       "      <td>8.5389</td>\n",
       "      <td>0.03</td>\n",
       "      <td>395</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>38.29</td>\n",
       "      <td>23.0640</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20630</th>\n",
       "      <td>100</td>\n",
       "      <td>200</td>\n",
       "      <td>-0.0032</td>\n",
       "      <td>-0.0005</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>643.85</td>\n",
       "      <td>1600.38</td>\n",
       "      <td>1432.14</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>8137.33</td>\n",
       "      <td>8.5036</td>\n",
       "      <td>0.03</td>\n",
       "      <td>396</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>38.37</td>\n",
       "      <td>23.0522</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20631 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0    1       2       3      4       5       6        7        8   \\\n",
       "0        1    1 -0.0007 -0.0004  100.0  518.67  641.82  1589.70  1400.60   \n",
       "1        1    2  0.0019 -0.0003  100.0  518.67  642.15  1591.82  1403.14   \n",
       "2        1    3 -0.0043  0.0003  100.0  518.67  642.35  1587.99  1404.20   \n",
       "3        1    4  0.0007  0.0000  100.0  518.67  642.35  1582.79  1401.87   \n",
       "4        1    5 -0.0019 -0.0002  100.0  518.67  642.37  1582.85  1406.22   \n",
       "...    ...  ...     ...     ...    ...     ...     ...      ...      ...   \n",
       "20626  100  196 -0.0004 -0.0003  100.0  518.67  643.49  1597.98  1428.63   \n",
       "20627  100  197 -0.0016 -0.0005  100.0  518.67  643.54  1604.50  1433.58   \n",
       "20628  100  198  0.0004  0.0000  100.0  518.67  643.42  1602.46  1428.18   \n",
       "20629  100  199 -0.0011  0.0003  100.0  518.67  643.23  1605.26  1426.53   \n",
       "20630  100  200 -0.0032 -0.0005  100.0  518.67  643.85  1600.38  1432.14   \n",
       "\n",
       "          9   ...       18      19    20   21    22     23     24       25  \\\n",
       "0      14.62  ...  8138.62  8.4195  0.03  392  2388  100.0  39.06  23.4190   \n",
       "1      14.62  ...  8131.49  8.4318  0.03  392  2388  100.0  39.00  23.4236   \n",
       "2      14.62  ...  8133.23  8.4178  0.03  390  2388  100.0  38.95  23.3442   \n",
       "3      14.62  ...  8133.83  8.3682  0.03  392  2388  100.0  38.88  23.3739   \n",
       "4      14.62  ...  8133.80  8.4294  0.03  393  2388  100.0  38.90  23.4044   \n",
       "...      ...  ...      ...     ...   ...  ...   ...    ...    ...      ...   \n",
       "20626  14.62  ...  8137.60  8.4956  0.03  397  2388  100.0  38.49  22.9735   \n",
       "20627  14.62  ...  8136.50  8.5139  0.03  395  2388  100.0  38.30  23.1594   \n",
       "20628  14.62  ...  8141.05  8.5646  0.03  398  2388  100.0  38.44  22.9333   \n",
       "20629  14.62  ...  8139.29  8.5389  0.03  395  2388  100.0  38.29  23.0640   \n",
       "20630  14.62  ...  8137.33  8.5036  0.03  396  2388  100.0  38.37  23.0522   \n",
       "\n",
       "       26  27  \n",
       "0     NaN NaN  \n",
       "1     NaN NaN  \n",
       "2     NaN NaN  \n",
       "3     NaN NaN  \n",
       "4     NaN NaN  \n",
       "...    ..  ..  \n",
       "20626 NaN NaN  \n",
       "20627 NaN NaN  \n",
       "20628 NaN NaN  \n",
       "20629 NaN NaN  \n",
       "20630 NaN NaN  \n",
       "\n",
       "[20631 rows x 28 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0023</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>643.02</td>\n",
       "      <td>1585.29</td>\n",
       "      <td>1398.21</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>8125.55</td>\n",
       "      <td>8.4052</td>\n",
       "      <td>0.03</td>\n",
       "      <td>392</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>38.86</td>\n",
       "      <td>23.3735</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.0027</td>\n",
       "      <td>-0.0003</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>641.71</td>\n",
       "      <td>1588.45</td>\n",
       "      <td>1395.42</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>8139.62</td>\n",
       "      <td>8.3803</td>\n",
       "      <td>0.03</td>\n",
       "      <td>393</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>39.02</td>\n",
       "      <td>23.3916</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>642.46</td>\n",
       "      <td>1586.94</td>\n",
       "      <td>1401.34</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>8130.10</td>\n",
       "      <td>8.4441</td>\n",
       "      <td>0.03</td>\n",
       "      <td>393</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>39.08</td>\n",
       "      <td>23.4166</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0042</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>642.44</td>\n",
       "      <td>1584.12</td>\n",
       "      <td>1406.42</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>8132.90</td>\n",
       "      <td>8.3917</td>\n",
       "      <td>0.03</td>\n",
       "      <td>391</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>39.00</td>\n",
       "      <td>23.3737</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0014</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>642.51</td>\n",
       "      <td>1587.19</td>\n",
       "      <td>1401.92</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>8129.54</td>\n",
       "      <td>8.4031</td>\n",
       "      <td>0.03</td>\n",
       "      <td>390</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>38.99</td>\n",
       "      <td>23.4130</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13091</th>\n",
       "      <td>100</td>\n",
       "      <td>194</td>\n",
       "      <td>0.0049</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>643.24</td>\n",
       "      <td>1599.45</td>\n",
       "      <td>1415.79</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>8213.28</td>\n",
       "      <td>8.4715</td>\n",
       "      <td>0.03</td>\n",
       "      <td>394</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>38.65</td>\n",
       "      <td>23.1974</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13092</th>\n",
       "      <td>100</td>\n",
       "      <td>195</td>\n",
       "      <td>-0.0011</td>\n",
       "      <td>-0.0001</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>643.22</td>\n",
       "      <td>1595.69</td>\n",
       "      <td>1422.05</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>8210.85</td>\n",
       "      <td>8.4512</td>\n",
       "      <td>0.03</td>\n",
       "      <td>395</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>38.57</td>\n",
       "      <td>23.2771</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13093</th>\n",
       "      <td>100</td>\n",
       "      <td>196</td>\n",
       "      <td>-0.0006</td>\n",
       "      <td>-0.0003</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>643.44</td>\n",
       "      <td>1593.15</td>\n",
       "      <td>1406.82</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>8217.24</td>\n",
       "      <td>8.4569</td>\n",
       "      <td>0.03</td>\n",
       "      <td>395</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>38.62</td>\n",
       "      <td>23.2051</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13094</th>\n",
       "      <td>100</td>\n",
       "      <td>197</td>\n",
       "      <td>-0.0038</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>643.26</td>\n",
       "      <td>1594.99</td>\n",
       "      <td>1419.36</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>8220.48</td>\n",
       "      <td>8.4711</td>\n",
       "      <td>0.03</td>\n",
       "      <td>395</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>38.66</td>\n",
       "      <td>23.2699</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13095</th>\n",
       "      <td>100</td>\n",
       "      <td>198</td>\n",
       "      <td>0.0013</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>642.95</td>\n",
       "      <td>1601.62</td>\n",
       "      <td>1424.99</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>8214.64</td>\n",
       "      <td>8.4903</td>\n",
       "      <td>0.03</td>\n",
       "      <td>396</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>38.70</td>\n",
       "      <td>23.1855</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13096 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0    1       2       3      4       5       6        7        8   \\\n",
       "0        1    1  0.0023  0.0003  100.0  518.67  643.02  1585.29  1398.21   \n",
       "1        1    2 -0.0027 -0.0003  100.0  518.67  641.71  1588.45  1395.42   \n",
       "2        1    3  0.0003  0.0001  100.0  518.67  642.46  1586.94  1401.34   \n",
       "3        1    4  0.0042  0.0000  100.0  518.67  642.44  1584.12  1406.42   \n",
       "4        1    5  0.0014  0.0000  100.0  518.67  642.51  1587.19  1401.92   \n",
       "...    ...  ...     ...     ...    ...     ...     ...      ...      ...   \n",
       "13091  100  194  0.0049  0.0000  100.0  518.67  643.24  1599.45  1415.79   \n",
       "13092  100  195 -0.0011 -0.0001  100.0  518.67  643.22  1595.69  1422.05   \n",
       "13093  100  196 -0.0006 -0.0003  100.0  518.67  643.44  1593.15  1406.82   \n",
       "13094  100  197 -0.0038  0.0001  100.0  518.67  643.26  1594.99  1419.36   \n",
       "13095  100  198  0.0013  0.0003  100.0  518.67  642.95  1601.62  1424.99   \n",
       "\n",
       "          9   ...       18      19    20   21    22     23     24       25  \\\n",
       "0      14.62  ...  8125.55  8.4052  0.03  392  2388  100.0  38.86  23.3735   \n",
       "1      14.62  ...  8139.62  8.3803  0.03  393  2388  100.0  39.02  23.3916   \n",
       "2      14.62  ...  8130.10  8.4441  0.03  393  2388  100.0  39.08  23.4166   \n",
       "3      14.62  ...  8132.90  8.3917  0.03  391  2388  100.0  39.00  23.3737   \n",
       "4      14.62  ...  8129.54  8.4031  0.03  390  2388  100.0  38.99  23.4130   \n",
       "...      ...  ...      ...     ...   ...  ...   ...    ...    ...      ...   \n",
       "13091  14.62  ...  8213.28  8.4715  0.03  394  2388  100.0  38.65  23.1974   \n",
       "13092  14.62  ...  8210.85  8.4512  0.03  395  2388  100.0  38.57  23.2771   \n",
       "13093  14.62  ...  8217.24  8.4569  0.03  395  2388  100.0  38.62  23.2051   \n",
       "13094  14.62  ...  8220.48  8.4711  0.03  395  2388  100.0  38.66  23.2699   \n",
       "13095  14.62  ...  8214.64  8.4903  0.03  396  2388  100.0  38.70  23.1855   \n",
       "\n",
       "       26  27  \n",
       "0     NaN NaN  \n",
       "1     NaN NaN  \n",
       "2     NaN NaN  \n",
       "3     NaN NaN  \n",
       "4     NaN NaN  \n",
       "...    ..  ..  \n",
       "13091 NaN NaN  \n",
       "13092 NaN NaN  \n",
       "13093 NaN NaN  \n",
       "13094 NaN NaN  \n",
       "13095 NaN NaN  \n",
       "\n",
       "[13096 rows x 28 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>112</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>99 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    112 \n",
       "0     98\n",
       "1     69\n",
       "2     82\n",
       "3     91\n",
       "4     93\n",
       "..   ...\n",
       "94   137\n",
       "95    82\n",
       "96    59\n",
       "97   117\n",
       "98    20\n",
       "\n",
       "[99 rows x 1 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_rul"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train.drop(columns=[26, 27], inplace=True)\n",
    "data_test.drop(columns=[26, 27], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unit_ID</th>\n",
       "      <th>cycles</th>\n",
       "      <th>setting_1</th>\n",
       "      <th>setting_2</th>\n",
       "      <th>setting_3</th>\n",
       "      <th>T2</th>\n",
       "      <th>T24</th>\n",
       "      <th>T30</th>\n",
       "      <th>T50</th>\n",
       "      <th>P2</th>\n",
       "      <th>...</th>\n",
       "      <th>phi</th>\n",
       "      <th>NRf</th>\n",
       "      <th>NRc</th>\n",
       "      <th>BPR</th>\n",
       "      <th>farB</th>\n",
       "      <th>htBleed</th>\n",
       "      <th>Nf_dmd</th>\n",
       "      <th>PCNfR_dmd</th>\n",
       "      <th>W31</th>\n",
       "      <th>W32</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>20631.000000</td>\n",
       "      <td>20631.000000</td>\n",
       "      <td>20631.000000</td>\n",
       "      <td>20631.000000</td>\n",
       "      <td>20631.0</td>\n",
       "      <td>20631.00</td>\n",
       "      <td>20631.000000</td>\n",
       "      <td>20631.000000</td>\n",
       "      <td>20631.000000</td>\n",
       "      <td>2.063100e+04</td>\n",
       "      <td>...</td>\n",
       "      <td>20631.000000</td>\n",
       "      <td>20631.000000</td>\n",
       "      <td>20631.000000</td>\n",
       "      <td>20631.000000</td>\n",
       "      <td>2.063100e+04</td>\n",
       "      <td>20631.000000</td>\n",
       "      <td>20631.0</td>\n",
       "      <td>20631.0</td>\n",
       "      <td>20631.000000</td>\n",
       "      <td>20631.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>51.506568</td>\n",
       "      <td>108.807862</td>\n",
       "      <td>-0.000009</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>642.680934</td>\n",
       "      <td>1590.523119</td>\n",
       "      <td>1408.933782</td>\n",
       "      <td>1.462000e+01</td>\n",
       "      <td>...</td>\n",
       "      <td>521.413470</td>\n",
       "      <td>2388.096152</td>\n",
       "      <td>8143.752722</td>\n",
       "      <td>8.442146</td>\n",
       "      <td>3.000000e-02</td>\n",
       "      <td>393.210654</td>\n",
       "      <td>2388.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>38.816271</td>\n",
       "      <td>23.289705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>29.227633</td>\n",
       "      <td>68.880990</td>\n",
       "      <td>0.002187</td>\n",
       "      <td>0.000293</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.500053</td>\n",
       "      <td>6.131150</td>\n",
       "      <td>9.000605</td>\n",
       "      <td>1.776400e-15</td>\n",
       "      <td>...</td>\n",
       "      <td>0.737553</td>\n",
       "      <td>0.071919</td>\n",
       "      <td>19.076176</td>\n",
       "      <td>0.037505</td>\n",
       "      <td>1.387812e-17</td>\n",
       "      <td>1.548763</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.180746</td>\n",
       "      <td>0.108251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.008700</td>\n",
       "      <td>-0.000600</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>641.210000</td>\n",
       "      <td>1571.040000</td>\n",
       "      <td>1382.250000</td>\n",
       "      <td>1.462000e+01</td>\n",
       "      <td>...</td>\n",
       "      <td>518.690000</td>\n",
       "      <td>2387.880000</td>\n",
       "      <td>8099.940000</td>\n",
       "      <td>8.324900</td>\n",
       "      <td>3.000000e-02</td>\n",
       "      <td>388.000000</td>\n",
       "      <td>2388.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>38.140000</td>\n",
       "      <td>22.894200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>26.000000</td>\n",
       "      <td>52.000000</td>\n",
       "      <td>-0.001500</td>\n",
       "      <td>-0.000200</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>642.325000</td>\n",
       "      <td>1586.260000</td>\n",
       "      <td>1402.360000</td>\n",
       "      <td>1.462000e+01</td>\n",
       "      <td>...</td>\n",
       "      <td>520.960000</td>\n",
       "      <td>2388.040000</td>\n",
       "      <td>8133.245000</td>\n",
       "      <td>8.414900</td>\n",
       "      <td>3.000000e-02</td>\n",
       "      <td>392.000000</td>\n",
       "      <td>2388.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>38.700000</td>\n",
       "      <td>23.221800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>52.000000</td>\n",
       "      <td>104.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>642.640000</td>\n",
       "      <td>1590.100000</td>\n",
       "      <td>1408.040000</td>\n",
       "      <td>1.462000e+01</td>\n",
       "      <td>...</td>\n",
       "      <td>521.480000</td>\n",
       "      <td>2388.090000</td>\n",
       "      <td>8140.540000</td>\n",
       "      <td>8.438900</td>\n",
       "      <td>3.000000e-02</td>\n",
       "      <td>393.000000</td>\n",
       "      <td>2388.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>38.830000</td>\n",
       "      <td>23.297900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>77.000000</td>\n",
       "      <td>156.000000</td>\n",
       "      <td>0.001500</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>643.000000</td>\n",
       "      <td>1594.380000</td>\n",
       "      <td>1414.555000</td>\n",
       "      <td>1.462000e+01</td>\n",
       "      <td>...</td>\n",
       "      <td>521.950000</td>\n",
       "      <td>2388.140000</td>\n",
       "      <td>8148.310000</td>\n",
       "      <td>8.465600</td>\n",
       "      <td>3.000000e-02</td>\n",
       "      <td>394.000000</td>\n",
       "      <td>2388.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>38.950000</td>\n",
       "      <td>23.366800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>100.000000</td>\n",
       "      <td>362.000000</td>\n",
       "      <td>0.008700</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>644.530000</td>\n",
       "      <td>1616.910000</td>\n",
       "      <td>1441.490000</td>\n",
       "      <td>1.462000e+01</td>\n",
       "      <td>...</td>\n",
       "      <td>523.380000</td>\n",
       "      <td>2388.560000</td>\n",
       "      <td>8293.720000</td>\n",
       "      <td>8.584800</td>\n",
       "      <td>3.000000e-02</td>\n",
       "      <td>400.000000</td>\n",
       "      <td>2388.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>39.430000</td>\n",
       "      <td>23.618400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            unit_ID        cycles     setting_1     setting_2  setting_3  \\\n",
       "count  20631.000000  20631.000000  20631.000000  20631.000000    20631.0   \n",
       "mean      51.506568    108.807862     -0.000009      0.000002      100.0   \n",
       "std       29.227633     68.880990      0.002187      0.000293        0.0   \n",
       "min        1.000000      1.000000     -0.008700     -0.000600      100.0   \n",
       "25%       26.000000     52.000000     -0.001500     -0.000200      100.0   \n",
       "50%       52.000000    104.000000      0.000000      0.000000      100.0   \n",
       "75%       77.000000    156.000000      0.001500      0.000300      100.0   \n",
       "max      100.000000    362.000000      0.008700      0.000600      100.0   \n",
       "\n",
       "             T2           T24           T30           T50            P2  ...  \\\n",
       "count  20631.00  20631.000000  20631.000000  20631.000000  2.063100e+04  ...   \n",
       "mean     518.67    642.680934   1590.523119   1408.933782  1.462000e+01  ...   \n",
       "std        0.00      0.500053      6.131150      9.000605  1.776400e-15  ...   \n",
       "min      518.67    641.210000   1571.040000   1382.250000  1.462000e+01  ...   \n",
       "25%      518.67    642.325000   1586.260000   1402.360000  1.462000e+01  ...   \n",
       "50%      518.67    642.640000   1590.100000   1408.040000  1.462000e+01  ...   \n",
       "75%      518.67    643.000000   1594.380000   1414.555000  1.462000e+01  ...   \n",
       "max      518.67    644.530000   1616.910000   1441.490000  1.462000e+01  ...   \n",
       "\n",
       "                phi           NRf           NRc           BPR          farB  \\\n",
       "count  20631.000000  20631.000000  20631.000000  20631.000000  2.063100e+04   \n",
       "mean     521.413470   2388.096152   8143.752722      8.442146  3.000000e-02   \n",
       "std        0.737553      0.071919     19.076176      0.037505  1.387812e-17   \n",
       "min      518.690000   2387.880000   8099.940000      8.324900  3.000000e-02   \n",
       "25%      520.960000   2388.040000   8133.245000      8.414900  3.000000e-02   \n",
       "50%      521.480000   2388.090000   8140.540000      8.438900  3.000000e-02   \n",
       "75%      521.950000   2388.140000   8148.310000      8.465600  3.000000e-02   \n",
       "max      523.380000   2388.560000   8293.720000      8.584800  3.000000e-02   \n",
       "\n",
       "            htBleed   Nf_dmd  PCNfR_dmd           W31           W32  \n",
       "count  20631.000000  20631.0    20631.0  20631.000000  20631.000000  \n",
       "mean     393.210654   2388.0      100.0     38.816271     23.289705  \n",
       "std        1.548763      0.0        0.0      0.180746      0.108251  \n",
       "min      388.000000   2388.0      100.0     38.140000     22.894200  \n",
       "25%      392.000000   2388.0      100.0     38.700000     23.221800  \n",
       "50%      393.000000   2388.0      100.0     38.830000     23.297900  \n",
       "75%      394.000000   2388.0      100.0     38.950000     23.366800  \n",
       "max      400.000000   2388.0      100.0     39.430000     23.618400  \n",
       "\n",
       "[8 rows x 26 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns_train = ['unit_ID','cycles','setting_1','setting_2','setting_3','T2','T24','T30','T50','P2','P15','P30','Nf',\n",
    "           'Nc','epr','Ps30','phi','NRf','NRc','BPR','farB','htBleed','Nf_dmd','PCNfR_dmd','W31','W32' ]\n",
    "data_train.columns = columns_train\n",
    "data_test.columns = columns_train\n",
    "data_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute RUL for a single unit\n",
    "def compute_rul(data):\n",
    "    max_cycles = data['cycles'].max()\n",
    "    data['RUL'] = max_cycles - data['cycles']\n",
    "    return data\n",
    "\n",
    "# Apply the RUL computation to each group\n",
    "train = data_train.groupby('unit_ID', group_keys=False).apply(compute_rul)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unit_ID</th>\n",
       "      <th>cycles</th>\n",
       "      <th>setting_1</th>\n",
       "      <th>setting_2</th>\n",
       "      <th>setting_3</th>\n",
       "      <th>T2</th>\n",
       "      <th>T24</th>\n",
       "      <th>T30</th>\n",
       "      <th>T50</th>\n",
       "      <th>P2</th>\n",
       "      <th>...</th>\n",
       "      <th>NRf</th>\n",
       "      <th>NRc</th>\n",
       "      <th>BPR</th>\n",
       "      <th>farB</th>\n",
       "      <th>htBleed</th>\n",
       "      <th>Nf_dmd</th>\n",
       "      <th>PCNfR_dmd</th>\n",
       "      <th>W31</th>\n",
       "      <th>W32</th>\n",
       "      <th>RUL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.0007</td>\n",
       "      <td>-0.0004</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>641.82</td>\n",
       "      <td>1589.70</td>\n",
       "      <td>1400.60</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>2388.02</td>\n",
       "      <td>8138.62</td>\n",
       "      <td>8.4195</td>\n",
       "      <td>0.03</td>\n",
       "      <td>392</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>39.06</td>\n",
       "      <td>23.4190</td>\n",
       "      <td>191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0019</td>\n",
       "      <td>-0.0003</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>642.15</td>\n",
       "      <td>1591.82</td>\n",
       "      <td>1403.14</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>2388.07</td>\n",
       "      <td>8131.49</td>\n",
       "      <td>8.4318</td>\n",
       "      <td>0.03</td>\n",
       "      <td>392</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>39.00</td>\n",
       "      <td>23.4236</td>\n",
       "      <td>190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>-0.0043</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>642.35</td>\n",
       "      <td>1587.99</td>\n",
       "      <td>1404.20</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>2388.03</td>\n",
       "      <td>8133.23</td>\n",
       "      <td>8.4178</td>\n",
       "      <td>0.03</td>\n",
       "      <td>390</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>38.95</td>\n",
       "      <td>23.3442</td>\n",
       "      <td>189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0007</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>642.35</td>\n",
       "      <td>1582.79</td>\n",
       "      <td>1401.87</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>2388.08</td>\n",
       "      <td>8133.83</td>\n",
       "      <td>8.3682</td>\n",
       "      <td>0.03</td>\n",
       "      <td>392</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>38.88</td>\n",
       "      <td>23.3739</td>\n",
       "      <td>188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>-0.0019</td>\n",
       "      <td>-0.0002</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>642.37</td>\n",
       "      <td>1582.85</td>\n",
       "      <td>1406.22</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>2388.04</td>\n",
       "      <td>8133.80</td>\n",
       "      <td>8.4294</td>\n",
       "      <td>0.03</td>\n",
       "      <td>393</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>38.90</td>\n",
       "      <td>23.4044</td>\n",
       "      <td>187</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   unit_ID  cycles  setting_1  setting_2  setting_3      T2     T24      T30  \\\n",
       "0        1       1    -0.0007    -0.0004      100.0  518.67  641.82  1589.70   \n",
       "1        1       2     0.0019    -0.0003      100.0  518.67  642.15  1591.82   \n",
       "2        1       3    -0.0043     0.0003      100.0  518.67  642.35  1587.99   \n",
       "3        1       4     0.0007     0.0000      100.0  518.67  642.35  1582.79   \n",
       "4        1       5    -0.0019    -0.0002      100.0  518.67  642.37  1582.85   \n",
       "\n",
       "       T50     P2  ...      NRf      NRc     BPR  farB  htBleed  Nf_dmd  \\\n",
       "0  1400.60  14.62  ...  2388.02  8138.62  8.4195  0.03      392    2388   \n",
       "1  1403.14  14.62  ...  2388.07  8131.49  8.4318  0.03      392    2388   \n",
       "2  1404.20  14.62  ...  2388.03  8133.23  8.4178  0.03      390    2388   \n",
       "3  1401.87  14.62  ...  2388.08  8133.83  8.3682  0.03      392    2388   \n",
       "4  1406.22  14.62  ...  2388.04  8133.80  8.4294  0.03      393    2388   \n",
       "\n",
       "   PCNfR_dmd    W31      W32  RUL  \n",
       "0      100.0  39.06  23.4190  191  \n",
       "1      100.0  39.00  23.4236  190  \n",
       "2      100.0  38.95  23.3442  189  \n",
       "3      100.0  38.88  23.3739  188  \n",
       "4      100.0  38.90  23.4044  187  \n",
       "\n",
       "[5 rows x 27 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_rul(sequence_length, max_rul=None):\n",
    "    \"\"\"\n",
    "    Calculate the Remaining Useful Life (RUL) for a given sequence length.\n",
    "    \n",
    "    Parameters:\n",
    "        sequence_length (int): Total number of time steps in the sequence.\n",
    "        max_rul (int, optional): Maximum cap for the RUL. If not specified, full RUL is returned.\n",
    "    \n",
    "    Returns:\n",
    "        np.ndarray: Array containing RUL values for the sequence.\n",
    "    \"\"\"\n",
    "    if max_rul is not None and sequence_length > max_rul:\n",
    "        # Apply RUL cap\n",
    "        capped_values = np.full(sequence_length - max_rul, max_rul)\n",
    "        descending_values = np.arange(max_rul - 1, -1, -1)\n",
    "        return np.concatenate([capped_values, descending_values])\n",
    "    else:\n",
    "        # Return standard descending RUL\n",
    "        return np.arange(sequence_length - 1, -1, -1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sliding_windows(data, labels=None, window_size=1, step=1):\n",
    "    \"\"\"\n",
    "    Generate sliding windows from input data and optionally associated labels.\n",
    "    \n",
    "    Parameters:\n",
    "        data (np.ndarray): Input data array with shape (num_samples, num_features).\n",
    "        labels (np.ndarray, optional): Target labels corresponding to the input data. Defaults to None.\n",
    "        window_size (int): Length of each sliding window. Defaults to 1.\n",
    "        step (int): Step size for the sliding window. Defaults to 1.\n",
    "    \n",
    "    Returns:\n",
    "        np.ndarray: Sliding windows of input data.\n",
    "        np.ndarray (optional): Sliding windows of labels, if provided.\n",
    "    \"\"\"\n",
    "    total_windows = (len(data) - window_size) // step + 1\n",
    "    feature_count = data.shape[1]\n",
    "    \n",
    "    # Prepare output arrays for sliding windows\n",
    "    windows = np.empty((total_windows, window_size, feature_count), dtype=np.float32)\n",
    "    if labels is not None:\n",
    "        label_windows = np.empty(total_windows, dtype=np.float32)\n",
    "    \n",
    "    for i in range(total_windows):\n",
    "        start_idx = i * step\n",
    "        end_idx = start_idx + window_size\n",
    "        windows[i] = data[start_idx:end_idx]\n",
    "        \n",
    "        if labels is not None:\n",
    "            label_windows[i] = labels[end_idx - 1]\n",
    "    \n",
    "    if labels is None:\n",
    "        return windows\n",
    "    else:\n",
    "        return windows, label_windows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_test_windows(engine_data, window_size, step, max_windows=1):\n",
    "    \"\"\"\n",
    "    Prepare sliding window batches from test data for a single engine.\n",
    "    \n",
    "    Parameters:\n",
    "        engine_data (np.ndarray): Test data for a single engine, shape (num_samples, num_features).\n",
    "        window_size (int): Length of each sliding window.\n",
    "        step (int): Step size for the sliding window.\n",
    "        max_windows (int, optional): Maximum number of windows to extract. Defaults to 1.\n",
    "    \n",
    "    Returns:\n",
    "        np.ndarray: Batches of sliding windows from the test data.\n",
    "        int: Actual number of windows extracted.\n",
    "    \"\"\"\n",
    "    total_windows = (len(engine_data) - window_size) // step + 1\n",
    "\n",
    "    if total_windows < max_windows:\n",
    "        # Not enough windows; extract as many as possible\n",
    "        required_length = (total_windows - 1) * step + window_size\n",
    "        extracted_windows = generate_sliding_windows(\n",
    "            data=engine_data[-required_length:], \n",
    "            labels=None, \n",
    "            window_size=window_size, \n",
    "            step=step\n",
    "        )\n",
    "        return extracted_windows, total_windows\n",
    "    else:\n",
    "        # Extract the desired number of windows\n",
    "        required_length = (max_windows - 1) * step + window_size\n",
    "        extracted_windows = generate_sliding_windows(\n",
    "            data=engine_data[-required_length:], \n",
    "            labels=None, \n",
    "            window_size=window_size, \n",
    "            step=step\n",
    "        )\n",
    "        return extracted_windows, max_windows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv(test1_path, sep=\"\\s+\", header=None, names=columns_train)\n",
    "true_rul = pd.read_csv(rul1_path, sep=\"\\s+\", header=None)\n",
    "\n",
    "window_size = 30\n",
    "step = 1\n",
    "max_rul = 125\n",
    "processed_train_data = []\n",
    "processed_train_targets = []\n",
    "max_windows = 5\n",
    "processed_test_data = []\n",
    "num_test_windows_list = []\n",
    "\n",
    "columns_to_be_dropped = ['unit_ID', 'setting_1', 'setting_2', 'setting_3', 'T2', 'P2', 'P15', 'epr', 'farB', 'Nf_dmd', 'PCNfR_dmd']\n",
    "train_data_first_column = data_train['unit_ID']\n",
    "test_data_first_column = test_data['unit_ID']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "scalar = StandardScaler()\n",
    "train_data = scalar.fit_transform(data_train.drop(columns = columns_to_be_dropped))\n",
    "test_data = scalar.transform(test_data.drop(columns=columns_to_be_dropped))\n",
    "\n",
    "train_data = pd.DataFrame(data = np.c_[train_data_first_column, train_data])\n",
    "test_data = pd.DataFrame(data = np.c_[test_data_first_column, test_data])\n",
    "\n",
    "num_train_machines = len(train_data[0].unique())\n",
    "num_test_machines = len(test_data[0].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in np.arange(1, num_train_machines + 1):\n",
    "    temp_train_data = train_data[train_data[0] == i].drop(columns = [0]).values\n",
    "\n",
    "    if (len(temp_train_data) < window_size):\n",
    "        print(\"Train engine {} doesn't have enough data for window_length of {}\".format(i, window_size))\n",
    "        raise AssertionError(\"Window length is larger than number of data points for some engines. \"\n",
    "                             \"Try decreasing window length.\")\n",
    "    \n",
    "    temp_train_targets = calculate_rul(sequence_length=temp_train_data.shape[0], max_rul=max_rul)\n",
    "\n",
    "    data_for_a_machine, targets_for_a_machine = generate_sliding_windows(temp_train_data, temp_train_targets, window_size=window_size, step=step)\n",
    "    processed_train_data.append(data_for_a_machine)\n",
    "    processed_train_targets.append(targets_for_a_machine)\n",
    "\n",
    "processed_train_data = np.concatenate(processed_train_data)\n",
    "processed_train_targets = np.concatenate(processed_train_targets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in np.arange(1, num_test_machines + 1):\n",
    "    temp_test_data = test_data[test_data[0] == i].drop(columns = [0]).values\n",
    "    \n",
    "    # Determine whether it is possible to extract test data with the specified window length.\n",
    "    if (len(temp_test_data) < window_size):\n",
    "        print(\"Test engine {} doesn't have enough data for window_length of {}\".format(i, window_size))\n",
    "        raise AssertionError(\"Window length is larger than number of data points for some engines. \"\n",
    "                             \"Try decreasing window length.\")\n",
    "    \n",
    "    # Prepare test data\n",
    "\n",
    "    test_data_for_an_engine, num_windows = prepare_test_windows(temp_test_data, window_size=window_size, step = step,\n",
    "                                                              max_windows = max_windows)\n",
    "    \n",
    "    processed_test_data.append(test_data_for_an_engine)\n",
    "    num_test_windows_list.append(num_windows)\n",
    "\n",
    "processed_test_data = np.concatenate(processed_test_data)\n",
    "true_rul = true_rul[0].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Processed trianing data shape: \", processed_train_data.shape)\n",
    "print(\"Processed training ruls shape: \", processed_train_targets.shape)\n",
    "print(\"Processed test data shape: \", processed_test_data.shape)\n",
    "print(\"True RUL shape: \", true_rul.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(processed_train_data, processed_train_targets, test_size = 0.2, random_state = 83)\n",
    "\n",
    "print(\"Processed train data shape: \", X_train.shape)\n",
    "print(\"Processed validation data shape: \", X_val.shape)\n",
    "print(\"Processed train targets shape: \", y_train.shape)\n",
    "print(\"Processed validation targets shape: \", y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "\n",
    "# Assuming `processed_train_data` and `processed_train_targets` are NumPy arrays\n",
    "processed_train_data = torch.tensor(processed_train_data, dtype=torch.float32)\n",
    "processed_train_targets = torch.tensor(processed_train_targets, dtype=torch.float32)\n",
    "\n",
    "# Splitting the dataset\n",
    "dataset = TensorDataset(processed_train_data, processed_train_targets)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=128, shuffle=False)\n",
    "\n",
    "print(\"Processed train data shape: \", processed_train_data[:train_size].shape)\n",
    "print(\"Processed validation data shape: \", processed_train_data[train_size:].shape)\n",
    "print(\"Processed train targets shape: \", processed_train_targets[:train_size].shape)\n",
    "print(\"Processed validation targets shape: \", processed_train_targets[train_size:].shape)\n",
    "\n",
    "# Define the PyTorch model\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_sizes, output_size):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.lstm1 = nn.LSTM(input_size, hidden_sizes[0], batch_first=True, num_layers=2)\n",
    "        self.lstm2 = nn.LSTM(hidden_sizes[0], hidden_sizes[1], batch_first=True)\n",
    "        self.lstm3 = nn.LSTM(hidden_sizes[1], hidden_sizes[2], batch_first=True)\n",
    "        self.fc1 = nn.Linear(hidden_sizes[2], 96)\n",
    "        self.fc2 = nn.Linear(96, 128)\n",
    "        self.fc3 = nn.Linear(128, output_size)\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x, _ = self.lstm1(x)\n",
    "        x, _ = self.lstm2(x)\n",
    "        x, (hn, _) = self.lstm3(x)\n",
    "        x = self.relu(self.fc1(hn[-1]))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Instantiate the model\n",
    "input_size = 15\n",
    "# hidden_sizes = [128, 64, 32]\n",
    "hidden_sizes = [128, 64, 32]\n",
    "epochs = 100\n",
    "output_size = 1\n",
    "model = LSTMModel(input_size, hidden_sizes, output_size)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Learning rate scheduler\n",
    "# def scheduler(epoch, optimizer):\n",
    "#     if epoch < 5:\n",
    "#         for param_group in optimizer.param_groups:\n",
    "#             param_group['lr'] = 0.001\n",
    "#     else:\n",
    "#         for param_group in optimizer.param_groups:\n",
    "#             param_group['lr'] = 0.0001\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n",
    "\n",
    "# Training loop\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    # scheduler(epoch, optimizer)\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(X_batch)\n",
    "        loss = criterion(y_pred.squeeze(), y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    scheduler.step()\n",
    "    \n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in val_loader:\n",
    "            y_pred = model(X_batch)\n",
    "            val_loss += criterion(y_pred.squeeze(), y_batch).item()\n",
    "    val_loss /= len(val_loader)\n",
    "    \n",
    "    print(f\"Epoch {epoch + 1}/{epochs}, Train Loss: {loss.item():.4f}, Validation Loss: {val_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import torch\n",
    "\n",
    "# Assuming 'model' is the trained PyTorch model and 'processed_test_data' is the prepared test data as a PyTorch tensor\n",
    "# Also assuming num_test_windows_list and true_rul are available\n",
    "processed_test_data = torch.tensor(processed_test_data, dtype=torch.float32)\n",
    "# Convert test data to a DataLoader\n",
    "test_loader = torch.utils.data.DataLoader(processed_test_data, batch_size=128, shuffle=False)\n",
    "\n",
    "# Move model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Collect predictions\n",
    "rul_pred = []\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        # batch = batch.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        output = model(batch).cpu().numpy().reshape(-1)\n",
    "        rul_pred.extend(output)\n",
    "\n",
    "# Split predictions based on the number of windows for each engine\n",
    "preds_for_each_engine = np.split(rul_pred, np.cumsum(num_test_windows_list)[:-1])\n",
    "\n",
    "# Compute the mean prediction for each engine, weighted by the number of windows\n",
    "mean_pred_for_each_engine = [\n",
    "    np.average(ruls_for_each_engine, weights=np.repeat(1 / num_windows, num_windows))\n",
    "    for ruls_for_each_engine, num_windows in zip(preds_for_each_engine, num_test_windows_list)\n",
    "]\n",
    "\n",
    "# Compute RMSE\n",
    "RMSE = np.sqrt(mean_squared_error(true_rul, mean_pred_for_each_engine))\n",
    "print(\"RMSE:\", RMSE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orinial_RMSE = RMSE\n",
    "indices_of_last_examples = np.cumsum(num_test_windows_list) - 1\n",
    "preds_for_last_example = np.concatenate(preds_for_each_engine)[indices_of_last_examples]\n",
    "RMSE_new = np.sqrt(mean_squared_error(true_rul, preds_for_last_example))\n",
    "print(\"RMSE (Only last examples): \", RMSE_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(true_rul, label = \"True RUL\", color = \"orange\")\n",
    "plt.plot(preds_for_last_example, label= \"Pred RUL\", color = \"blue\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Define parameters\n",
    "window_length = 30\n",
    "shift = 1\n",
    "columns_to_be_dropped = ['unit_ID', 'setting_1', 'setting_2', 'setting_3', 'T2', 'P2', 'P15', 'epr', 'farB', 'Nf_dmd', 'PCNfR_dmd']\n",
    "\n",
    "# Preprocess test data\n",
    "test_data_first_column = data_test['unit_ID']\n",
    "test_data = scalar.transform(data_test.drop(columns=columns_to_be_dropped))  # Normalize test data\n",
    "test_data = pd.DataFrame(data=np.c_[test_data_first_column, test_data])\n",
    "\n",
    "# Prepare test data sequences\n",
    "processed_test_data = []\n",
    "num_test_windows_list = []\n",
    "for unit_id in np.unique(test_data[0]):\n",
    "    temp_test_data = test_data[test_data[0] == unit_id].drop(columns=[0]).values\n",
    "    if len(temp_test_data) < window_length:\n",
    "        print(f\"Test engine {unit_id} doesn't have enough data for window_length of {window_length}\")\n",
    "        continue\n",
    "    test_data_for_engine, num_windows = prepare_test_windows(temp_test_data, window_size=window_size, step=step)\n",
    "    processed_test_data.append(test_data_for_engine)\n",
    "    num_test_windows_list.append(num_windows)\n",
    "\n",
    "processed_test_data = np.concatenate(processed_test_data)  # Combine all engines' data\n",
    "print(f\"Processed test data shape: {processed_test_data.shape}\")\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_test_tensor = torch.tensor(processed_test_data, dtype=torch.float32)\n",
    "\n",
    "# Create DataLoader\n",
    "test_loader = DataLoader(TensorDataset(X_test_tensor), batch_size=1, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Training function for a single model\n",
    "def train_model(model, train_loader, val_loader, epochs=20, lr=0.001):\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n",
    "\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = model(X_batch)\n",
    "            loss = criterion(y_pred.squeeze(), y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "        scheduler.step()\n",
    "        train_loss /= len(train_loader)\n",
    "        \n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in val_loader:\n",
    "                y_pred = model(X_batch)\n",
    "                val_loss += criterion(y_pred.squeeze(), y_batch).item()\n",
    "        val_loss /= len(val_loader)\n",
    "        \n",
    "        print(f\"Epoch {epoch + 1}/{epochs}, Train Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Ensemble of models\n",
    "def train_ensemble(train_loader, val_loader, input_size, hidden_sizes, output_size, n_models=5, epochs=20):\n",
    "    models = []\n",
    "    for i in range(n_models):\n",
    "        print(f\"Training model {i + 1}/{n_models}\")\n",
    "        model = LSTMModel(input_size, hidden_sizes, output_size)\n",
    "        trained_model = train_model(model, train_loader, val_loader, epochs)\n",
    "        models.append(trained_model)\n",
    "    return models\n",
    "\n",
    "# Generate predictions and uncertainty\n",
    "def predict_ensemble(models, test_loader):\n",
    "    all_predictions = []\n",
    "    for model in models:\n",
    "        model.eval()\n",
    "        predictions = []\n",
    "        with torch.no_grad():\n",
    "            for X_batch in test_loader:\n",
    "                # Extract the tensor from the tuple\n",
    "                X_batch = X_batch[0]\n",
    "                preds = model(X_batch).squeeze().cpu().numpy()\n",
    "                # Wrap scalar predictions in a 1D array\n",
    "                predictions.append(np.array([preds]))\n",
    "        # Concatenate predictions into a single array for this model\n",
    "        all_predictions.append(np.concatenate(predictions))\n",
    "    \n",
    "    # Convert to numpy array: shape (n_models, n_samples)\n",
    "    all_predictions = np.array(all_predictions)\n",
    "    mean_prediction = np.mean(all_predictions, axis=0)\n",
    "    std_prediction = np.std(all_predictions, axis=0)\n",
    "    \n",
    "    return mean_prediction, std_prediction\n",
    "\n",
    "\n",
    "# Plot results\n",
    "def plot_results(true_rul, mean_prediction, std_prediction):\n",
    "    x = np.arange(len(true_rul))\n",
    "    \n",
    "    plt.figure(figsize=(20, 8))\n",
    "    plt.plot(x, true_rul, label=\"True RUL\", color=\"blue\")\n",
    "    plt.plot(x, mean_prediction, label=\"Mean Prediction\", color=\"red\")\n",
    "    plt.fill_between(\n",
    "        x, \n",
    "        mean_prediction - std_prediction, \n",
    "        mean_prediction + std_prediction, \n",
    "        color=\"orange\", \n",
    "        alpha=0.3, \n",
    "        label=\"Uncertainty (±1 STD)\"\n",
    "    )\n",
    "    plt.xlabel(\"Sample Index\")\n",
    "    plt.ylabel(\"RUL\")\n",
    "    plt.legend()\n",
    "    plt.title(\"Ensemble Predictions with Uncertainty\")\n",
    "    plt.show()\n",
    "\n",
    "# Example usage\n",
    "# input_size = next(iter(train_loader))[0].shape[2]\n",
    "\n",
    "hidden_sizes = [128, 64, 32]\n",
    "output_size = 1\n",
    "\n",
    "# Create DataLoaders\n",
    "# train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "# val_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)\n",
    "# test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train ensemble\n",
    "n_models = 5\n",
    "ensemble_models = train_ensemble(train_loader, val_loader, input_size, hidden_sizes, output_size, n_models)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming true_rul contains the true RUL values for the test set\n",
    "true_rul_tensor = torch.tensor(true_rul, dtype=torch.float32)  # Convert true RUL to tensor\n",
    "\n",
    "# Create TensorDataset with both inputs and targets\n",
    "test_dataset = TensorDataset(X_test_tensor, true_rul_tensor)\n",
    "\n",
    "# Create DataLoader\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import torch\n",
    "\n",
    "# Assuming true_rul, test_loader, num_test_windows_list, and ensemble_models are available\n",
    "true_rul_tensor = torch.tensor(true_rul, dtype=torch.float32)  # Convert true RUL to tensor\n",
    "\n",
    "# Move ensemble models to evaluation mode and collect predictions\n",
    "ensemble_predictions = []\n",
    "\n",
    "for model in ensemble_models:\n",
    "    model.eval()\n",
    "    model_preds = []\n",
    "    with torch.no_grad():\n",
    "        for X_batch, _ in test_loader:  # Extract test inputs (ignore labels)\n",
    "            X_batch = X_batch\n",
    "            output = model(X_batch).numpy().reshape(-1)\n",
    "            # output = model(X_batch).cpu().numpy().reshape(-1)\n",
    "            model_preds.extend(output)\n",
    "    ensemble_predictions.append(model_preds)\n",
    "\n",
    "# Convert to numpy array (shape: n_models, n_samples)\n",
    "ensemble_predictions = np.array(ensemble_predictions)\n",
    "\n",
    "# Average predictions across the ensemble\n",
    "rul_pred = ensemble_predictions.mean(axis=0)\n",
    "\n",
    "# Split predictions based on the number of windows for each engine\n",
    "preds_for_each_engine = np.split(rul_pred, np.cumsum(num_test_windows_list)[:-1])\n",
    "\n",
    "# Compute the mean prediction for each engine, weighted by the number of windows\n",
    "mean_pred_for_each_engine = [\n",
    "    np.average(ruls_for_each_engine, weights=np.repeat(1 / num_windows, num_windows))\n",
    "    for ruls_for_each_engine, num_windows in zip(preds_for_each_engine, num_test_windows_list)\n",
    "]\n",
    "\n",
    "# Compute RMSE\n",
    "RMSE = np.sqrt(mean_squared_error(true_rul, mean_pred_for_each_engine))\n",
    "print(\"RMSE (Ensemble Mean Prediction):\", RMSE)\n",
    "\n",
    "# Compute RMSE using the last prediction from each engine\n",
    "indices_of_last_examples = np.cumsum(num_test_windows_list) - 1\n",
    "preds_for_last_example = np.concatenate(preds_for_each_engine)[indices_of_last_examples]\n",
    "RMSE_last = np.sqrt(mean_squared_error(true_rul, preds_for_last_example))\n",
    "print(\"RMSE (Only Last Examples):\", RMSE_last)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions and uncertainty\n",
    "mean_pred, std_pred = predict_ensemble(ensemble_models, test_loader)\n",
    "\n",
    "# Access true RUL from the dataset\n",
    "true_rul = true_rul_tensor.numpy()\n",
    "\n",
    "# Plot results\n",
    "plot_results(true_rul, mean_pred, std_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM MultiSwag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim.swa_utils as swa_utils\n",
    "\n",
    "# SWAG training function\n",
    "def train_swag_model(model, train_loader, val_loader, epochs=epochs, lr=0.001, swa_start=15):\n",
    "    \"\"\"\n",
    "    Train a model using SWAG (Stochastic Weight Averaging Gaussian).\n",
    "\n",
    "    Args:\n",
    "        model: PyTorch model to train.\n",
    "        train_loader: DataLoader for training data.\n",
    "        val_loader: DataLoader for validation data.\n",
    "        epochs: Total number of epochs to train.\n",
    "        lr: Learning rate.\n",
    "        swa_start: Epoch to start stochastic weight averaging.\n",
    "\n",
    "    Returns:\n",
    "        swa_model: SWAG model.\n",
    "        swa_optimizer: SWAG optimizer.\n",
    "    \"\"\"\n",
    "    # Initialize standard optimizer\n",
    "    base_optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    # Initialize SWA optimizer\n",
    "    swa_model = swa_utils.AveragedModel(model)\n",
    "    swa_scheduler = optim.lr_scheduler.CosineAnnealingLR(base_optimizer, T_max=epochs)\n",
    "\n",
    "    criterion = nn.MSELoss()\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            base_optimizer.zero_grad()\n",
    "            y_pred = model(X_batch)\n",
    "            loss = criterion(y_pred.squeeze(), y_batch)\n",
    "            loss.backward()\n",
    "            base_optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        train_loss /= len(train_loader)\n",
    "        \n",
    "        # Apply SWA updates\n",
    "        if epoch >= swa_start:\n",
    "            swa_model.update_parameters(model)\n",
    "            swa_scheduler.step()\n",
    "        \n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in val_loader:\n",
    "                y_pred = model(X_batch)\n",
    "                val_loss += criterion(y_pred.squeeze(), y_batch).item()\n",
    "        val_loss /= len(val_loader)\n",
    "        \n",
    "        print(f\"Epoch {epoch + 1}/{epochs}, Train Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}\")\n",
    "    \n",
    "    # Update SWA BatchNorm statistics\n",
    "    swa_utils.update_bn(train_loader, swa_model)\n",
    "    return swa_model, base_optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_multi_swag(train_loader, val_loader, input_size, hidden_sizes, output_size, n_models=5, epochs=epochs, lr=0.001, swa_start=int(0.75*epochs)):\n",
    "    \"\"\"\n",
    "    Train multiple SWAG models for MultiSWAG.\n",
    "\n",
    "    Args:\n",
    "        train_loader: DataLoader for training data.\n",
    "        val_loader: DataLoader for validation data.\n",
    "        input_size: Input feature size.\n",
    "        hidden_sizes: List of hidden layer sizes.\n",
    "        output_size: Output size.\n",
    "        n_models: Number of SWAG models to train.\n",
    "        epochs: Number of epochs for training.\n",
    "        lr: Learning rate.\n",
    "        swa_start: Epoch to start stochastic weight averaging.\n",
    "\n",
    "    Returns:\n",
    "        models: List of trained SWAG models.\n",
    "    \"\"\"\n",
    "    models = []\n",
    "    for i in range(n_models):\n",
    "        print(f\"Training SWAG model {i + 1}/{n_models}\")\n",
    "        model = LSTMModel(input_size, hidden_sizes, output_size)\n",
    "        swa_model, _ = train_swag_model(model, train_loader, val_loader, epochs, lr, swa_start)\n",
    "        models.append(swa_model)\n",
    "    return models\n",
    "\n",
    "# Generate predictions for MultiSWAG\n",
    "def predict_multi_swag(models, test_loader):\n",
    "    all_predictions = []\n",
    "    for model in models:\n",
    "        model.eval()\n",
    "        predictions = []\n",
    "        with torch.no_grad():\n",
    "            for X_batch in test_loader:\n",
    "                X_batch = X_batch[0]  # Extract tensor from tuple\n",
    "                preds = model(X_batch).squeeze().cpu().numpy()\n",
    "                predictions.append(np.array([preds]))\n",
    "        all_predictions.append(np.concatenate(predictions))\n",
    "    \n",
    "    # Convert to numpy array: shape (n_models, n_samples)\n",
    "    all_predictions = np.array(all_predictions)\n",
    "    mean_prediction = np.mean(all_predictions, axis=0)\n",
    "    std_prediction = np.std(all_predictions, axis=0)\n",
    "    \n",
    "    return mean_prediction, std_prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_swag_models = train_multi_swag(train_loader, val_loader, input_size, hidden_sizes, output_size, n_models=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_pred, std_pred = predict_multi_swag(multi_swag_models, test_loader)\n",
    "plot_results(true_rul, mean_pred, std_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "engine",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
